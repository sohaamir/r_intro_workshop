---
title: "Basic data analysis and plotting using R"
subtitle: "Using tidyverse and ggplot2 to analyse and plot data"
format: html
---

Now that you have a basic understanding how how `R` works, and what it is used for, let's put our newly learned skills to practical use.

When working with data in our studies, we can consider `R` to be the central software for data management, analysis and plotting. It is quite robust for this purpose, in that it also reads in data of many different types and from many different sources. 

## Working with behavioural data in R

In this section we will directly work with some participant data from the HAVEN study.

HAVEN was a study investigating the relationship between cognition, neurovascular health and platelet function in older adults. For the purposes of this workshop, we will be using a subset of the data from the study, specifically focusing on simple demographical and behavioural data.

We will firstly plot the data using a variety of plots to understand how certain graphs are preferred for certain data. We will then perform basic statistical tests to understand whether the relationship between the variables are significant.

### Load and display the data

Install packages (only run this if you haven't restored the environment)

```{r, message=FALSE, warning=FALSE}}
rm(list=ls())

install.packages("tidyverse", "dplyr")
library(tidyverse)
library(dplyr)
```

First thing first, we need to load and read the data. The first dataset is in the `participant_data.csv` file. This file format is called 'comma separated values' or CSV, which we can read into R using the `read_csv` function. We firstly assign the data to a variable called `sub_data` (short for subject data), which we can now work directly with.

In the code below, we are downloading the data from a GitHub repository and saving it to our working directory. We can then read it in using the `read.csv` function from `tidyverse`. The first two lines are only required for the online workshop where we run code in the browser; in `R` you can just run the bottom line.

```{r}
#| context: interactive

url <- "https://raw.githubusercontent.com/sohaamir/r_intro_workshop/refs/heads/main/data/participant_data.csv"
download.file(url, "participant_data.csv")
sub_data <- read.csv("participant_data.csv")
```

It is good practice to name your variables appropriately. For example, I assigned the data to `sub_data` instead of `data` because `data` is a reserved word in R. This means that if you try to use `data` as a variable name, it may overwrite the built-in function `data()`, which can cause confusion and errors in your code. An alternative is to use `df` short for data frame, or `dat` (short for data) but these are too generic and don't provide any information about the data (which is particularly useful when working with multiple datasets). So, I would recommend using `sub_data` or `participant_data` instead.


Just to see if we have loaded in the correct file and correctly assigned it, we can display the first few rows of `sub_data` using the `head` function. This is a pre-built function in `R` which will show the first 6 rows of the data. You can also specify how many rows you want to see by adding a number in brackets, e.g., `head(sub_data, 10)` will show the first 10 rows.

```{r}
#| context: interactive
head(sub_data)
```

So you can see we have a variety of different variables, including `age`, `gender`, `height`, `weight`, `bmi` and `lesion_volume`.

We can calculate summary statistics for each variable using the `summary` function. The summary statistics will give us the mean, range etc. for each.

```{r}
#| context: interactive
summary(sub_data)
```

And we now that to access a specific column, we can use the `$` operator. For example, if we wanted to see the age of each participant, we can run:
```{r}
#| context: interactive
summary(sub_data$age)
```

## Data Visualization using `ggplot2()`

An important practice when working with data is to understand what the data looks like. This is important for several reasons, including understanding the distribution of the data, identifying outliers, and understanding the relationships between different variables - even basic!

For example, if we wanted to understand whether people who are older have a higher lesion volume (possibly as a result of aging) we can create a scatter plot of age and total lesion volume, and add a line of best fit. We will use the `ggplot2` package, which - as mentioned - is a powerful and flexible plotting system.

### Scatter plot

Let's create a scatter plot of age and lesion volume:
```{r}
#| context: interactive

ggplot(sub_data, aes(x = age, y = `lesion_volume`)) +
  geom_point(color = "darkblue") +
  geom_smooth(method = "lm", se = TRUE, color = "darkred", linetype = "solid") +
  labs(x = "age",
       y = "lesion volume") +
  theme_minimal()
```

Here we are specifically using the `ggplot2` package using the `geom_point` function to create the points, and the `geom_smooth` function to add a line of best fit. The `method = "lm"` argument specifies that we want to use a simple linear regression model for the line of best fit.

So, you can see a positive correlation, namely that (generally) the older you are, the higher the total lesion volume will be.

If we wanted to understand whether there is a difference between the height and weight between males and females, we can also do this using a scatter plot, but grouped by gender:
```{r}
#| context: interactive

ggplot(sub_data, aes(x=height, y=weight, colour=gender)) + 
  geom_point() +
    labs(x = "Height (m)",
         y = "Weight (kg)") +
  theme_minimal()
```

The `colour` argument within `aes` separates the data by the variable assigned. You can see that males are generally towards the top and right (i.e., taller and heavier).

### Boxplot

To see the specific differences between males and females, we can create a boxplot, which visualizes the mean and quartiles:
```{r}
#| context: interactive

# Plot 1: Height by gender
ggplot(sub_data, aes(x = gender, y = height, fill = gender)) +
  geom_boxplot(width = 0.3) + 
  stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "red") +
  labs(x = "gender",
       y = "Height (m)") +
  theme_minimal()

# Plot 2: Weight by gender
ggplot(sub_data, aes(x = gender, y = weight, fill = gender)) +
  geom_boxplot(width = 0.3) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "red") +
  labs(x = "gender",
       y = "Weight (kg)") +
  theme_minimal()
```

Again, we use `ggplot2` to structure the plot, and the `geom_boxplot` function to actually create the violin component. You can see that indeed, men are taller and weigh more than women in the group.

### Histogram

Histograms are useful if we want to understand the spread of a particular variable across the group. For example, if we wanted to see the distribution of BMI across the group, we can use the `geom_histogram` function. This will create a histogram of the BMI variable, and - using the `geom_density` argument we can also add a density curve to it.

Let's plot a histogram of BMI:

```{r}
#| context: interactive

ggplot(sub_data, aes(x = bmi)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 1, fill = "steelblue", color = "black") +
  geom_density(color = "darkred", linetype = "solid", linewidth = 1) +
  labs(x = "BMI",
       y = "Density") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5),
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold"))
```

So you can see that the most common BMI is 25, with the variable normally distributed.

## Basic statistics and significance testing

So we have been able to visualise the data, and now have a general idea of some of the trends. But how can we test whether these relationships are significant? Can we statistically say that men are taller than women? Can we say that the older you are the higher lesion volume you have?

To answer these questions, lets firstly get some more summary statistics, this time for height and weight specificially. We firstly group the data by gender and run the `summarise` function to calculate the mean and standard deviation for each group.

```{r}
#| context: interactive

group_by(sub_data, gender) %>%
  summarise(
    count = n(),
    mean = mean(height, na.rm = TRUE),
    sd = sd(height, na.rm = TRUE)
  )

group_by(sub_data, gender) %>%
  summarise(
    count = n(),
    mean = mean(weight, na.rm = TRUE),
    sd = sd(weight, na.rm = TRUE)
  )
```

You can see that the mean height for women (n=29) is 1.64 and men (n=23) is 1.80 (meters), whilst the mean weight is 65.9 and 85.5 (kg) respectively.

### T-test

But is this difference significant? To find out the statistical difference between the mean of two groups we can simply run a t-test. We do this using the `t.test` function, which will calculate the t-statistic and p-value for us. The default is a two-tailed t-test, which tests whether the means of two groups are significantly different from each other without direction. We can also run a one-tailed t-test, but we need to specify the direction of the difference.
```{r}
#| context: interactive

# filter the data
males <- sub_data[sub_data$gender == "M", ]
females <- sub_data[sub_data$gender == "F", ]

# Two-tailed t-test (when you don't know the direction of the difference)

# Two-tailed test for height and weight
t.test(males$height, females$height)
t.test(males$weight, females$weight)

# One-tailed t-test (when you have a hypothesis for the direction of the difference)

# One-tailed test for height (females > males)
t.test(females$height, males$height, alternative = "greater")

# One-tailed test for weight (females > males)
t.test(females$weight, males$weight, alternative = "greater")
```

So, we can see in either case, the p-values that there is a statistically significant difference between the height and weight between males and females. (9.77e-10 is a 0 followed by 10 more zeroes so 0.000000000977). We can also see that the p-value for the one-tailed test where we test the alternative hypothesis that women weigh more than men is 1. This makes sense, as this is not the case!

### Correlation

Similarly, if we want to see whether there is a significant relationship between two continuous variables, i.e., between age and lesion volume across the entire group, we can run a correlation. We can do this using the `cor.test` function, which will give us the correlation coefficient (r) and the corresponding p-value.

Conversely to the `t.test` function, when we run the `cor.test` it actually generates a list of values. The `str` function will show this structure.
```{r}
#| context: interactive

# Perform correlation test between age and lesion volume
cor_result <- cor.test(sub_data$age, sub_data$lesion_volume)

# Explore the structure of cor_result
str(cor_result)
```

You can also see that in addition to the various statistics, it also mentions the method, which is Pearson's product-moment correlation. This is the default method used in `R`, but you can also use Spearman's rank correlation or other correlation methods by changing the `method` argument.

Using the `names` function, we can print the data recorded, and see that it contains several components, including the correlation coefficient (r), p-value, confidence intervals and t-statistic.
```{r}
#| context: interactive

# List all components available in cor_result
names(cor_result)
```

We can then extract the important values (the R and p-value) from the `cor_result` object using the `$` operator.
```{r}
#| context: interactive

r_value <- cor_result$estimate
p_value <- cor_result$p.value

print(p_value)
print(r_value)
```

And we can re-plot this using `ggplot` as we did above, but also adding the r and p-values to the plot:
```{r}
#| context: interactive

# Calculate linear regression
lesion_regression <- lm(lesion_volume ~ age, data = sub_data)
r_value <- cor(sub_data$age, sub_data$lesion_volume)
p_value <- summary(lesion_regression)$coefficients[2, "Pr(>|t|)"]

# Create plot
ggplot(sub_data, aes(x = age, y = lesion_volume)) +
  geom_point(color = "darkblue") +
  geom_smooth(method = "lm", se = TRUE, color = "darkred", linetype = "solid") +
  labs(x = "Age",
       y = "Lesion Volume") +
  theme_minimal() +
  annotate("text", x = min(sub_data$age), y = max(sub_data$lesion_volume),
           label = paste("R = ", round(r_value, 2), "\np-value = ", format.pval(p_value, digits = 2), sep = ""),
           hjust = 0, vjust = 1)
```

So, whilst there is a significant relationship between age and lesion volume (excluding other predictors), we can see one or two outliers in our data. What would the data look like if we removed them?

We need to remove those with lesion volumes greater than 5. In `R` there are often several ways to perform the same operation, here are some examples which we can use:
```r
# Remove rows where lesion_volume exceeds 5
data_filtered <- sub_data[sub_data$lesion_volume <= 5, ]

# Alternatively, using dplyr
library(dplyr)
sub_data_filtered <- sub_data %>%
  filter(lesion_volume <= 5)

# Or using the subset function
sub_data_filtered <- subset(sub_data, lesion_volume <= 5)
```

Let's use the first approach and then plot the relationship with a line of best fit:
```{r}
#| context: interactive

# Remove rows where lesion_volume exceeds 5
sub_data_filtered <- sub_data[sub_data$lesion_volume <= 5, ]

# Perform linear regression
model <- lm(lesion_volume ~ age, data = sub_data_filtered)
p_value <- summary(model)$coefficients[2, "Pr(>|t|)"]

# Create a scatter plot
ggplot(sub_data_filtered, aes(x = age, y = lesion_volume)) +
  geom_point(color = "darkblue") +
  geom_smooth(method = "lm", se = TRUE, color = "darkred", linetype = "solid") +
  labs(x = "Age",
       y = "Lesion Volume") +
  scale_y_continuous(limits = c(0, 5)) +
  theme_minimal() +
  annotate("text", x = min(sub_data_filtered$age), y = 5,
           label = paste("R = ", round(r_value, 2), "\np-value = ", format.pval(p_value, digits = 2), sep = ""),
           hjust = 0, vjust = 1)
```

The linear regression model `lm(lesion_volume ~ age, data = sub_data_filtered)` is equivalent to a correlation test which we can see by running the `cor.test` function again on the filtered data.
```{r}
#| context: interactive

# Perform correlation test between age and lesion volume
cor_result <- cor.test(sub_data_filtered$age, sub_data_filtered$lesion_volume)

# Extract correlation coefficient and p-value
r_value <- cor_result$estimate
p_value <- cor_result$p.value

print(p_value)
print(r_value)
```

So, the relationship is still significant, but less so than before.

Finally, we may also want to see if there is a difference between males and females with lesion volume and age. If so, we would expect to see a significant difference in the correlations between the two groups. Let's run a correlation for males and females with lesion volume separately, and plot them on the same graph:

```{r}
#| context: interactive

# Perform linear regression for males and females separately
model_male <- lm(`lesion_volume` ~ age, data = sub_data_filtered[sub_data_filtered$gender == "M",])
model_female <- lm(`lesion_volume` ~ age, data = sub_data_filtered[sub_data_filtered$gender == "F",])

# Extract p-values
p_value_male <- summary(model_male)$coefficients[2, "Pr(>|t|)"]
p_value_female <- summary(model_female)$coefficients[2, "Pr(>|t|)"]

# Create a scatter plot separating Males and Females
ggplot(sub_data_filtered, aes(x = age, y = lesion_volume, color = gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, linetype = "solid") +
  labs(title = "Scatter plot of age and Total lesion volume by gender",
       x = "age",
       y = "lesion_volume") +
  scale_y_continuous(limits = c(0, 5)) +
  theme_minimal() +
  annotate("text", x = min(sub_data_filtered$age), y = 5,
           label = paste("Males: p-value = ", format.pval(p_value_male, digits = 2), "\nFemales: p-value = ", format.pval(p_value_female, digits = 2), sep = ""),
           hjust = 0, vjust = 1)
```

We can now see that there is only a significant difference in the relationship between age and total lesion volume for females, and not for males.

## Regression models

In the HAVEN study, we were interested with understanding the potential influence of demographical and brain factors upon memory function. To do this we can look at participants' reaction times and accuracy during an episodic memory task using the `rt_data` dataset. 

The episodic memory task was a recall task, where about an hour prior, participants watched a video (actually an episode of the TV series 'Outnumbered'). Inside the MRI scanner, they were presented with two pictures, representing scenes from the episode, and there were asked to select the one which happened first.

Let's load in the data:
```{r}
#| context: interactive

rm(list=ls())

url <- "https://raw.githubusercontent.com/sohaamir/r_intro_workshop/refs/heads/main/data/rt_data.csv"
download.file(url, "rt_data.csv")
rt_data <- read.csv("rt_data.csv")
```

Run `head` to see the data:
```{r}
#| context: interactive

head(rt_data)
```

And `summary ` to see the summary statistics:
```{r}
#| context: interactive

summary(rt_data)
```

You can see that it is essentially an expanded version of the previous dataset, which includes accuracy and mean reaction time for correct and incorrect responses. Our goal is to create a model to describe how various factors influence participants' performance on the task.

As before, let's plot the relationship between age and performance on the task, both for the reaction time (only for ones they got correct) and the percentage accuracy.

Remember, before running any modelling analyses, it is always a good idea to look at your data and it's relationships, even it is rudimentary!

We can do this using `ggplot2` and the `lm` function to create a simple linear model. We will also add a line of best fit to the plot, and annotate it with the p-value from the model. Note that this time the plotting code is significantly longer than before. This is to demonstrate how you can customise your plots to make them look more suitable for a scientific publication. The previous code I kept purposely simple to make it easier to understand and manipulate.

In this code, we specifically create a custom theme for the plot, which increases the font size and removes the grid lines. 
```{r}
#| context: interactive

# Create a theme for scientific publication
theme_publication <- theme_minimal() +
  theme(
    axis.title = element_text(size = 20),
    axis.text = element_text(size = 20),
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.line = element_line(color = "black", size = 0.5)
  )

# Plot Age against correct_rt
rt_model <- lm(correct_rt ~ age, data = rt_data)
rt_corr <- cor.test(rt_data$age, rt_data$correct_rt)
rt_r_value <- rt_corr$estimate
rt_p_value <- rt_corr$p.value

rt_plot <- ggplot(rt_data, aes(x = age, y = correct_rt)) +
  geom_point(color = "#4682B4", size = 3, alpha = 0.4) +
  geom_smooth(method = "lm", se = TRUE, color = "#1E3A8A", fill = "#4682B4", alpha = 0.2) +
  labs(x = "Age",
       y = "Mean Correct Reaction Time (ms)") +
  scale_x_continuous(breaks = c(50, 60, 70, 80), limits = c(50, 80)) +
  theme_publication +
  annotate("text", x = max(rt_data$age) * 0.7, y = max(rt_data$correct_rt) * 0.9,
           label = sprintf("r = %.3f, p = %.3f", rt_r_value, rt_p_value),
           hjust = 0, vjust = 1, linewidth = 5)

# Plot Age against correct_percent (green color scheme)
percent_model <- lm(correct_percent ~ age, data = rt_data)
percent_corr <- cor.test(rt_data$age, rt_data$correct_percent)
percent_r_value <- percent_corr$estimate
percent_p_value <- percent_corr$p.value

percent_plot <- ggplot(rt_data, aes(x = age, y = correct_percent)) +
  geom_point(color = "#228B22", size = 3, alpha = 0.4) +
  geom_smooth(method = "lm", se = TRUE, color = "#006400", fill = "#228B22", alpha = 0.2) +
  labs(x = "Age",
       y = "Correct Percentage (%)") +
  scale_x_continuous(breaks = c(50, 60, 70, 80), limits = c(50, 80)) +
  theme_publication +
  annotate("text", x = max(rt_data$age) * 0.7, y = max(rt_data$correct_percent) * 0.9,
           label = sprintf("r = %.3f, p = %.3f", percent_r_value, percent_p_value),
           hjust = 0, vjust = 1, linewidth = 5)

# Display the plots
print(rt_plot)
print(percent_plot)
```

So we can see that older participants took longer (on correct trials), and made more mistakes. Both of these are significant.

But is there a difference between males and females? Let's plot the correct reaction time for both separately.

```{r}
#| context: interactive

## Calculate correlation and p-values for each gender
male_data <- rt_data[rt_data$gender == "M", ]
female_data <- rt_data[rt_data$gender == "F", ]

male_corr <- cor.test(male_data$age, male_data$correct_rt)
female_corr <- cor.test(female_data$age, female_data$correct_rt)

# Create plot with separate colors and regression lines for each gender
percent_plot_by_gender <- ggplot(rt_data, aes(x = age, y = correct_rt, color = gender, fill = gender)) +
  geom_point(size = 3, alpha = 0.4) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.2) +
  scale_color_manual(values = c("M" = "#1E90FF", "F" = "#ff6666")) +  # Blue for male, pink for female
  scale_fill_manual(values = c("M" = "#1E90FF", "F" = "#ff6666")) +
  labs(x = "Age",
       y = "Correct Percentage (%)",
       color = "gender",
       fill = "gender") +
  scale_x_continuous(breaks = c(50, 60, 70, 80), limits = c(50, 80)) +
  theme_publication +
  theme(legend.position = "right") +
  # Add annotation for male correlation
  annotate("text", x = max(rt_data$age) * 0.65, y = max(rt_data$correct_rt) * 0.95,
           label = sprintf("Male: r = %.3f, p = %.3f", male_corr$estimate, male_corr$p.value),
           hjust = 0, vjust = 1, size = 4.5, color = "#1E90FF") +
  # Add annotation for female correlation
  annotate("text", x = max(rt_data$age) * 0.65, y = max(rt_data$correct_rt) * 0.87,
           label = sprintf("Female: r = %.3f, p = %.3f", female_corr$estimate, female_corr$p.value),
           hjust = 0, vjust = -6, size = 4.5, color = "#ff6666")

print(percent_plot_by_gender)
```

Importantly, we can see a gender effect; whilst females show a significantly positive correlation between age and task performance, there is no such effect within males.

### Creating a linear regression model for task performance

Linear regression helps us understand how different independent variables ("predictors") influence a dependent (outcome) variable. In our case, we want to see how predictors influence task performance (accuracy).

Let's start with one predictor - age. The `lm` function - as previously used - creates a linear model, and the `str` function shows the structure of the model. 
```{r}
#| context: interactive

# Simple regression model
age_model <- lm(correct_rt ~ age, data = rt_data)

# See model structure
str(age_model)
```

You can see that a lot happens underneath the hood! Importantly, we can use the `summary` function to see the results of the model, including the coefficients, p-values and R-squared value.

```{r}
#| context: interactive

# View results
summary(age_model)
```

The coefficients tell us how much the dependent variable (correct reaction time) changes for each unit increase in the predictor variable (age). The p-value Pr(>|t|) tells us whether this relationship is statistically significant. Importantly, the age co-efficient is significant (p = 0.029) meaning that as people get older, their reaction times tend to increase.

To specifically test the influence of age and gender on performance - as indicated by the correlation plot, we need to add gender to our model. But there are a few things to note:

1) Because gender is a categorical variable, it should be implemented in the model as a factor. We actually do not need to do this explicitly, as the `lm` function will automatically convert it to a factor. However, it is good practice to do so, so we will do it explicitly as well.
2) We should add gender as an interaction term with age, rather than a separate fixed effect. This is because we are explicitly interested in whether two slopes are significantly different. Adding both as separate terms would just test the effect of each separately, and not the interaction.

Here is the code to create and run this model:
```{r}
#| context: interactive

# Convert gender to factor (not explicitly needed)
rt_data$gender <- factor(rt_data$gender, levels = c("F", "M"))

# Multiple regression model - * indicates interaction
age_gender_model <- lm(correct_rt ~ age * gender, data = rt_data)

# View results
summary(age_gender_model)
```

From the results, we can see that the interaction term is not significant (p = 0.249). This means that despite what we observed in our plot, the difference in how age affects reaction time between males and females is not statistically significant.

### Model selection

But which is the best model to use? We can run an ANOVA (analysis of variance) to compare the models. This will tell us whether the additional predictors significantly improve the model fit.
```{r}
#| context: interactive

anova(age_model, age_gender_model)
```

We can see from the output that compared to the simple age model, adding in gender as an interaction significantly improves the model fit (p = 0.035).

ANOVA - in this specific case - compared the models by measuring how much the prediction accuracy (through the residual sum of squares - RSS) increases when you add each new predictor. It then tests whether this is statistically significant using an F-test. A significant p-value means the added predictor significantly improves the model, while a non-significant p-value suggests it doesn't.

For a more formal test of model selection, we can use information criterion. The most common is the Akaike Information Criterion (AIC), which penalizes models for complexity. The model with the lowest AIC is considered the best fit. We can also use the Bayesian Information Criterion (BIC), which is similar but penalizes complexity more heavily.
```{r}
#| context: interactive

# Calculate AIC and BIC for both models
aic_age <- AIC(age_model)
aic_age_gender <- AIC(age_gender_model)

bic_age <- BIC(age_model)
bic_age_gender <- BIC(age_gender_model)

print(paste("AIC for age model:", aic_age))
print(paste("AIC for age + gender model:", aic_age_gender))
print(paste("BIC for age model:", aic_age))
print(paste("BIC for age + gender model:", aic_age_gender))
```

You can see that the AIC favours the more complex interaction model, whilst conversely, the BIC favors the simpler model. This is because the BIC penalizes complexity more heavily than the AIC. The choice of which model to use also depends on the specific context and goals of your analysis. In this case, because we are interested in specifically modeling the interaction effects, we can stick with that model.

## Calculating group differences using ANOVA and Tukey's HSD

ANOVAs are commonly used to determine whether there is a significant difference between multiple groups (>2). In our data, we know that there is a negative correlation between age and task performance, but is there a difference between age groups? We can use an ANOVA to test this.

Firstly, let's create the age groups, which will be subjects in their 50s, 60s and 70s. 
```{r}
#| context: interactive

# Create age groups
rt_data <- rt_data %>%
  mutate(age_group = case_when(
    age >= 50 & age < 60 ~ "50s",
    age >= 60 & age < 70 ~ "60s",
    age >= 70 & age < 80 ~ "70s"
  ))

# Check the grouping - get count, min and max age within each group
rt_data %>%
  group_by(age_group) %>%
  summarise(
    n = n(),
    min_age = min(age),
    max_age = max(age)
  )
```

The piping operator `%>%` is a powerful feature of the `dplyr` package that allows you to chain together multiple operations. It basically translates to 'and then'.

So in our code, we are creating a new column called `age_group` which is based on the age of the participants. We then group by these groups, and then summarise the data to get the count, minimum and maximum age within each group.

Let's now get the average accuracy within each group:

```{r}
#| context: interactive

# Calculate mean accuracy for each age group
rt_data %>%
  group_by(age_group) %>%
  summarise(mean_accuracy = mean(correct_percent, na.rm = TRUE))
```

Following the plot earlier, we can see that the average accuracy also decreases with age, even after splitting by age group. But is there a significant difference? We can run an ANOVA to test this using the `aov` function. This function takes two arguments, the variables consisting of the dependent variable (correct percentage) and the independent variable (the age groups), and the data.
```{r}
#| context: interactive

# Calculate ANOVA for accuracy across age groups
anova_model <- aov(correct_percent ~ age_group, data = rt_data)

# View results
summary(anova_model)
```

The results tell us that there is evidence that at least one age group has a significantly different mean accuracy from the others. But it doesn't tell us which one(s) specifically.

To do that, we can run a post-hoc test. We will run Tukey's HSD (Honestly Significant Difference) test, which is a common post-hoc test used after ANOVA specifically for this.
```{r}
#| context: interactive

# Perform Tukey's HSD post-hoc test
tukey_result <- TukeyHSD(anova_model)
print(tukey_result)

plot(tukey_result)
```
Contrary to the ANOVA, this tell us that there is no significant difference between any of the groups. There was a trend for the 50s group to be more accurate than the 70s group, but not significant (p=0.06). So, we can conclude that there is in fact no significant difference in accuracy between age groups.
